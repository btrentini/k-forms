{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffForms\n",
    "\n",
    "Train a neural network $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$ that outputs values over a point cloud in such a way that the function varies smoothly over the manifold formed by the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "1. Create an arbitrary point cloud:  \n",
    "\n",
    "   $ X \\in \\mathbb{R}^{N \\times D} $\n",
    "\n",
    "2. Build a **k-NN graph** (k=6) with Euclidean distances\n",
    "\n",
    "3. Compute the **heat kernel matrix**  \n",
    "\n",
    "   $\n",
    "   K_t(i,j) = \\frac{1}{(4\\pi t)^{d/2}} \\exp\\left( -\\frac{\\|x_i - x_j\\|^2}{4t} \\right)\n",
    "   $\n",
    "\n",
    "4. Compute a **discrete Laplacian** from the kernel (normalised or unnormalised)\n",
    "\n",
    "5. Train a model to **approximate a function** that minimises a Laplacian regularisation:  \n",
    "\n",
    "   $\n",
    "   \\mathcal{L}(f) = f^\\top L f = \\sum_{i,j} K_t(i,j) (f_i - f_j)^2\n",
    "   $\n",
    "\n",
    "_This encourages **smoothness** of \\( f \\) over the manifold._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "N = 100         # number of points\n",
    "D = 3           # dimension of each point\n",
    "k = 6           # k-NN\n",
    "t = 1.0         # heat kernel time\n",
    "\n",
    "# Step 1: Create random point cloud\n",
    "X = torch.randn(N, D)\n",
    "\n",
    "# Step 2: k-NN graph\n",
    "nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X)  # +1 includes the point itself\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "# Step 3: Build heat kernel matrix\n",
    "heat_kernel = torch.zeros(N, N)\n",
    "\n",
    "for i in range(N):\n",
    "    \"\"\"\n",
    "    How heat would diffuse from poin i to point j over time t. \n",
    "    This favours local interactions, and is closely related to the Gaussian kernel. \n",
    "    The kernel is symmetric and positive, giving rise to a weighted adjacency matrix for the graph.\n",
    "    \"\"\"\n",
    "    for j_idx, j in enumerate(indices[i][1:]):  # Skip self-loop\n",
    "        d2 = torch.norm(X[i] - X[j]) ** 2\n",
    "        coef = 1 / ((4 * np.pi * t) ** (D / 2))\n",
    "        heat_kernel[i, j] = coef * torch.exp(-d2 / (4 * t))\n",
    "        heat_kernel[j, i] = heat_kernel[i, j]  # symmetry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree matrix\n",
    "deg = torch.diag(heat_kernel.sum(dim=1))\n",
    "\n",
    "# Unnormalised Laplacian\n",
    "L = deg - heat_kernel\n",
    "\n",
    "# Normalised Laplacian\n",
    "# L = IDENTITY − deg^{-1/2} @ K @ D^{−1/2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By optimising the Laplacian loss, the model learns a function that respects the geometry of the data. \n",
    "\n",
    "This is useful in many contexts and connects ideas from spectral graph theory, differential geometry, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Laplacian loss = 0.120950\n",
      "Epoch 5: Laplacian loss = 0.020843\n",
      "Epoch 10: Laplacian loss = 0.011319\n",
      "Epoch 15: Laplacian loss = 0.007935\n",
      "Epoch 20: Laplacian loss = 0.004639\n",
      "Epoch 25: Laplacian loss = 0.002371\n",
      "Epoch 30: Laplacian loss = 0.001802\n",
      "Epoch 35: Laplacian loss = 0.001418\n",
      "Epoch 40: Laplacian loss = 0.000736\n",
      "Epoch 45: Laplacian loss = 0.000690\n",
      "Epoch 50: Laplacian loss = 0.000494\n",
      "Epoch 55: Laplacian loss = 0.000404\n",
      "Epoch 60: Laplacian loss = 0.000301\n",
      "Epoch 65: Laplacian loss = 0.000262\n",
      "Epoch 70: Laplacian loss = 0.000223\n",
      "Epoch 75: Laplacian loss = 0.000183\n",
      "Epoch 80: Laplacian loss = 0.000157\n",
      "Epoch 85: Laplacian loss = 0.000136\n",
      "Epoch 90: Laplacian loss = 0.000120\n",
      "Epoch 95: Laplacian loss = 0.000106\n",
      "Epoch 100: Laplacian loss = 0.000095\n",
      "Epoch 105: Laplacian loss = 0.000086\n",
      "Epoch 110: Laplacian loss = 0.000079\n",
      "Epoch 115: Laplacian loss = 0.000072\n",
      "Epoch 120: Laplacian loss = 0.000066\n",
      "Epoch 125: Laplacian loss = 0.000060\n",
      "Epoch 130: Laplacian loss = 0.000054\n",
      "Epoch 135: Laplacian loss = 0.000049\n",
      "Epoch 140: Laplacian loss = 0.000045\n",
      "Epoch 145: Laplacian loss = 0.000041\n",
      "Epoch 150: Laplacian loss = 0.000038\n",
      "Epoch 155: Laplacian loss = 0.000036\n",
      "Epoch 160: Laplacian loss = 0.000033\n",
      "Epoch 165: Laplacian loss = 0.000032\n",
      "Epoch 170: Laplacian loss = 0.000030\n",
      "Epoch 175: Laplacian loss = 0.000028\n",
      "Epoch 180: Laplacian loss = 0.000027\n",
      "Epoch 185: Laplacian loss = 0.000026\n",
      "Epoch 190: Laplacian loss = 0.000025\n",
      "Epoch 195: Laplacian loss = 0.000024\n",
      "Epoch 200: Laplacian loss = 0.000023\n",
      "Epoch 205: Laplacian loss = 0.000022\n",
      "Epoch 210: Laplacian loss = 0.000021\n",
      "Epoch 215: Laplacian loss = 0.000020\n",
      "Epoch 220: Laplacian loss = 0.000019\n",
      "Epoch 225: Laplacian loss = 0.000019\n",
      "Epoch 230: Laplacian loss = 0.000018\n",
      "Epoch 235: Laplacian loss = 0.000017\n",
      "Epoch 240: Laplacian loss = 0.000017\n",
      "Epoch 245: Laplacian loss = 0.000016\n",
      "Epoch 250: Laplacian loss = 0.000016\n",
      "Epoch 255: Laplacian loss = 0.000015\n",
      "Epoch 260: Laplacian loss = 0.000015\n",
      "Epoch 265: Laplacian loss = 0.000014\n",
      "Epoch 270: Laplacian loss = 0.000014\n",
      "Epoch 275: Laplacian loss = 0.000014\n",
      "Epoch 280: Laplacian loss = 0.000013\n",
      "Epoch 285: Laplacian loss = 0.000013\n",
      "Epoch 290: Laplacian loss = 0.000013\n",
      "Epoch 295: Laplacian loss = 0.000012\n",
      "Epoch 300: Laplacian loss = 0.000012\n",
      "Epoch 305: Laplacian loss = 0.000012\n",
      "Epoch 310: Laplacian loss = 0.000011\n",
      "Epoch 315: Laplacian loss = 0.000011\n",
      "Epoch 320: Laplacian loss = 0.000011\n",
      "Epoch 325: Laplacian loss = 0.000011\n",
      "Epoch 330: Laplacian loss = 0.000010\n",
      "Epoch 335: Laplacian loss = 0.000010\n",
      "Epoch 340: Laplacian loss = 0.000010\n",
      "Epoch 345: Laplacian loss = 0.000010\n",
      "Epoch 350: Laplacian loss = 0.000010\n",
      "Epoch 355: Laplacian loss = 0.000009\n",
      "Epoch 360: Laplacian loss = 0.000009\n",
      "Epoch 365: Laplacian loss = 0.000009\n",
      "Epoch 370: Laplacian loss = 0.000009\n",
      "Epoch 375: Laplacian loss = 0.000009\n",
      "Epoch 380: Laplacian loss = 0.000008\n",
      "Epoch 385: Laplacian loss = 0.000008\n",
      "Epoch 390: Laplacian loss = 0.000008\n",
      "Epoch 395: Laplacian loss = 0.000008\n",
      "Epoch 400: Laplacian loss = 0.000008\n",
      "Epoch 405: Laplacian loss = 0.000008\n",
      "Epoch 410: Laplacian loss = 0.000007\n",
      "Epoch 415: Laplacian loss = 0.000007\n",
      "Epoch 420: Laplacian loss = 0.000007\n",
      "Epoch 425: Laplacian loss = 0.000007\n",
      "Epoch 430: Laplacian loss = 0.000007\n",
      "Epoch 435: Laplacian loss = 0.000007\n",
      "Epoch 440: Laplacian loss = 0.000007\n",
      "Epoch 445: Laplacian loss = 0.000006\n",
      "Epoch 450: Laplacian loss = 0.000006\n",
      "Epoch 455: Laplacian loss = 0.000006\n",
      "Epoch 460: Laplacian loss = 0.000006\n",
      "Epoch 465: Laplacian loss = 0.000006\n",
      "Epoch 470: Laplacian loss = 0.000006\n",
      "Epoch 475: Laplacian loss = 0.000006\n",
      "Epoch 480: Laplacian loss = 0.000006\n",
      "Epoch 485: Laplacian loss = 0.000006\n",
      "Epoch 490: Laplacian loss = 0.000006\n",
      "Epoch 495: Laplacian loss = 0.000005\n"
     ]
    }
   ],
   "source": [
    "class DiffForms(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate model\n",
    "model = DiffForms(D, 32, 1)  # scalar-valued function on point cloud\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Laplacian smoothing loss\n",
    "def laplacian_loss(f_vals, L):\n",
    "    \"\"\"\n",
    "    This is the Dirichlet energy of f\n",
    "    It encourages neighbouring points with strong kernel affinity to have similar function values. \n",
    "    Minimising this loss promotes smoothness of f over the manifold.\n",
    "    \"\"\"\n",
    "    return torch.sum(f_vals.T @ L @ f_vals)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    f_vals = model(X) # f : R^D -> R\n",
    "    loss = laplacian_loss(f_vals, L)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: Laplacian loss = {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
